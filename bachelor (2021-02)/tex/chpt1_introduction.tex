% !TEX root = main.tex
% !TEX encoding = UTF-8

\chapter{Introduction}
\section{Background}
% This happens because the CPU is designed for generic serial operations, which is not suitable for matrix calculation, often required by DNN.

Recently, there is a demand for Deep Neural Network (DNN) applications in embedded systems. For DNN development's convenience, several DNN frameworks such as Tensorflow \cite{tensorflow} and PyTorch \cite{pytorch} have been developed. Some frameworks can accelerate computation by utilizing an external computing device rather than a CPU.
A CPU has a disadvantage when using it for DNN calculation. A CPU is designed for generic serial operations, which is not suitable for matrix calculation, often required by DNN.
Furthermore, a CPU's memory bandwidth is relatively slower than others, leading to a bottleneck issue. For example, a modern x86 CPU bandwidth using DDR4 SDRAM can have a maximum bandwidth of 35.2GB/s. However, the bandwidth of modern GPU using GDDR6X SDRAM can have a maximum bandwidth of 1TB/s (Reference \cite{curucialmemory,microngddr6}). For these reasons, a popular framework such as Tensorflow accelerates a calculation by utilizing GPU's high bandwidth memory and parallel computing power. However, frameworks that utilize non-GPU hardware such as FPGA are relatively rare.
There are frameworks developed for implementing a DNN inference application to embedded systems. For example, TensorFlow Lite for Microcontrollers \cite{tfl_m} allows running machine learning models with quantized parameters on devices with a few kilobytes of memory. Nevertheless, it does not support FPGA.


% can implement a DNN inference application to embedded systems.
% with quantized parameters and it allows running machine learning models on devices with only a few kilobytes of memory.
% Although, TensorFlow Lite for Microcontrollers \cite{tfl_m} can implement a DNN inference application to embedded systems.
% with quantized parameters and it allows running machine learning models on devices with only a few kilobytes of memory. It does not support FPGA.

\section{Purpose and contribution of the paper}

Since many frameworks do not support implementing a network to embedded systems with FPGA, we review implementing networks created and trained by existing DNN frameworks in this paper. Furthermore, we explore efficient hardware design of implementation. We choose Neural Network Console (NNC) as its implementation target. The main reason for choosing NNC is that NNC provides to quantize its parameters and data path between layers. NNC supports quantization at the stage of training. This feature is essential because calculating with floating-point arithmetic requires more memory and computation cost than an integer calculation. Furthermore, we can make a FPGA circuit size relatively small by utilizing lower bit quantization.

Moreover, NNC provides a user-friendly experience by utilizing its GUI. If we can automate a implementation of NNC's network, we expect NNC becomes a more productive tool. Therefore, by reviewing implementing networks generated by NNC, we believe this could be the first step for creating an automated tool.

%our final goal has set to make an automated tool that implements the network created by the existing DNN framework to FPGA.

% There are various studies about implementation of DNN on FPGA. DNNBuilder provides \cite{dnnbuilder}.

Implementation of FPGA using register-transfer level (RTL) language can make further optimization difficult and might be time-consuming. Therefore, we use a system-level design toolkit named SystemBuilder with high-level synthesis (HLS) for development \cite{honda2005systembuilder,honda2007rtos}. We will present more detail about this process in section \ref{sec:SystemBuilder}.

% We used various optimization techniques using HLS.
The contribution of this paper are as follows:
\begin{itemize}
    \item Review of the FPGA implementation of a network generated by Neural Network Console.
    \item A case study shows that inferences from implementation were comparable as the original network from the framework.
    \item Exploring an efficient hardware design using HLS.
    % We demonstrate the possibility of the implementation of existing DNN frameworks. We confirmed that inferences from implementation were the same as the original network. %, and also we tried to proofed the benefit of using FPGA by the advantages of FPGA's optimization.
\end{itemize}

We believe that contribution of this paper will help in the further development of a tool that automatically implements the network from NNC. We present this tool in \Figref{fig:autotool}.
