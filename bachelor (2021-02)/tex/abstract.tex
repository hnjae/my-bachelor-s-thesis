% !TEX root = main.tex
% !TEX encoding = UTF-8

\chapter*{Abstract}
  DNN (Deep Neural Network) implementation on an embedded system with FPGAs has been actively conducted.
Although several deployment tools for FPGA have been developed, such as PYNQ by Xilinx, DNN frameworks that generate trained networks are rare that can accelerate on FPGA.
Therefore, this paper aims at the FPGA implementation of networks generated in a DNN framework, which is not targeting FPGA.

In order to implement DNN networks on FPGA, the use of floating-point numbers should be avoided due to the increase of the computation cost and circuit size. Consequently, we can think of quantizing the data to an integer. However, in order for the inference of the quantized implemented network to be equivalent to the inference in the original framework, it is necessary to run quantization aware training at the framework. Therefore we choose Neural Network Console (NNC) as a target of implementation. NNC can quantize parameters and activations from the stage of training. It is expected that the inference result of NNC and the inference result of the network implemented in the FPGA can match.

In this paper, we implement networks generated by the NNC on FPGAs. Moreover, we also explore high-efficiency hardware design using High-Level Synthesis. In a case study, we implement three networks where all parameters and activations are quantized to 8-bit integers.


We compare two hardware designs to explore high-efficiency hardware design in the case study. The first design uses a pipeline execution of a loop statement operating matrix multiplication, while the second one utilizes pipelined layers of DNN. In DNN, the main consumption of inference time occurs in the multiplication and addition of matrices. As a result of the performance gains obtained in pipelined matrix multiplications, it is observed that there is a 35 percent of performance improvement by pipelining one matrix multiplication. At the same time, there is no significant difference in circuit size. The LUT usage rate increased by only 0.1 percent. As for the pipelined DNN layers, the result is a 3.5 percent of performance improvement and a 0.45 percent increase in LUT usage. Thus, the efficiency is not as great as the pipelined multiplication.

Implementing without using any floating-point arithmetic, we confirm that inferences' accuracy is comparable to the original network. Of the three networks implemented, two confirm that the inference results are equivalent to the original, and one matches 99.9 percent. In the model that does not match completely, six inferences out of 10,000 are different from the original. Finding out the cause of this error should be done in the future.

We expect this research to be a first step that makes NNC able to support more hardware platforms.  This implies we will be able to run a DNN model on various hardware.
